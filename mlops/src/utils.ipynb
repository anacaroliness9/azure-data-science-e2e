{"cells":[{"cell_type":"code","source":["# Databricks notebook source\nimport pandas as pd\nimport numpy as np\n# Azure libs\nfrom azureml.core.webservice import AciWebservice,  AksWebservice, Webservice\nfrom azureml.core.image import Image\nfrom azureml.core import Workspace\nfrom azureml.core.authentication import ServicePrincipalAuthentication\nfrom azureml.core.compute import AksCompute\nfrom azureml.exceptions import WebserviceException\nfrom azureml.core.model import Model, InferenceConfig\nfrom azureml.core.environment import Environment\nfrom azureml.core.conda_dependencies import CondaDependencies\n\n# SKLearn\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\n\n# MLFlow\nimport mlflow\nimport mlflow.sklearn\nimport mlflow.xgboost\nimport mlflow.azureml\nfrom mlflow.tracking.client import MlflowClient\nfrom mlflow.entities import ViewType\n\nimport shutil\nimport xgboost as xgb\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\ndef get_dataset(filename):\n  data = spark.read.parquet(filename)\n  return data.toPandas()\n\ndef preprocessing(dataset):\n  numeric_columns = []\n  for col in dataset.columns:\n    if(dataset[col].dtypes!='object'):\n      numeric_columns.append(col)\n \n  dataset = dataset.dropna()\n  return dataset, numeric_columns\n\ndef split_dataset(dataset, seed, test_size=0.33):\n  train_dataset, test_dataset = train_test_split(dataset, random_state=seed, test_size=test_size)\n  return train_dataset, test_dataset\n\ndef get_X_y(train, test, target_column, numeric_columns, drop_columns):\n  X_train = train[numeric_columns].drop(drop_columns, axis=1)\n  X_test = test[numeric_columns].drop(drop_columns, axis=1)\n\n  y_train = train[target_column]\n  y_test = test[target_column]\n  return X_train, X_test, y_train, y_test\n\ndef train_model(X_train, y_train, X_test, y_test):\n  mlflow.set_experiment('/churn-prediction')\n  \n  with mlflow.start_run(run_name='mlops-train') as run:\n    train = xgb.DMatrix(data=X_train, label=y_train)\n    test = xgb.DMatrix(data=X_test, label=y_test)\n    \n    # Pass in the test set so xgb can track an evaluation metric. XGBoost terminates training when the evaluation metric\n    # is no longer improving.\n    model = xgb.train(params=params, dtrain=train, num_boost_round=1000,\\\n                       evals=[(test, \"test\")], early_stopping_rounds=50)\n\n    mlflow.xgboost.log_model(model, 'model')\n    run_id = run.info.run_id\n\n  return \"runs:/\" + run_id + \"/model\"\n\ndef validate_model(model, X_test, y_test):\n  predictions_test = model.predict_proba(X_test)[:,1]\n  auc_score = roc_auc_score(y_test, predictions_test)\n  return auc_score\n\n# COMMAND ----------\n\ndef get_model_uri(experiment_name, run_name):\n  experiment = MlflowClient().get_experiment_by_name(experiment_name)\n  experiment_ids = eval('[' + experiment.experiment_id + ']')\n\n  query = f\"tag.mlflow.runName = '{run_name}'\"\n  run = MlflowClient().search_runs(experiment_ids, query, ViewType.ALL)[0]\n\n  return \"runs:/\" + run.info.run_id + \"/model\"\n\ndef load_model(model_uri):\n  model = mlflow.xgboost.load_model(model_uri)\n  return model\n\ndef persist_model(model, model_path):\n  shutil.rmtree(model_path)\n\n  # Persist the XGBoost model\n  mlflow.xgboost.save_model(model, model_path)\n\ndef register_model(workspace, model_name, model_description, model_path):\n  model_azure = Model.register(model_path = model_path,\n                               model_name = model_name,\n                               description = model_description,\n                               workspace = workspace,\n                               tags={})\n  return model_azure\n\ndef get_workspace(workspace_name, resource_group, subscription_id):\n  svc_pr = ServicePrincipalAuthentication(\n      tenant_id = dbutils.secrets.get(scope = \"azure-key-vault\", key = \"tenant-id\"),\n      service_principal_id = dbutils.secrets.get(scope = \"azure-key-vault\", key = \"client-id\"),\n      service_principal_password = dbutils.secrets.get(scope = \"azure-key-vault\", key = \"client-secret\"))\n\n  workspace = Workspace.get(name = workspace_name,\n                            resource_group = resource_group,\n                            subscription_id = subscription_id,\n                            auth=svc_pr)\n  \n  return workspace\n\ndef get_inference_config(environment_name, conda_file, entry_script):\n  # Create the environment\n  env = Environment(name=environment_name)\n\n  conda_dep = CondaDependencies(conda_file)\n\n  # Define the packages needed by the model and scripts\n  conda_dep.add_pip_package(\"azureml-defaults\")\n  conda_dep.add_pip_package(\"xgboost\")\n\n  # Adds dependencies to PythonSection of myenv\n  env.python.conda_dependencies=conda_dep\n\n  inference_config = InferenceConfig(entry_script=entry_script,\n                                     environment=env)\n  \n  return inference_config\n\ndef deploy_aci(workspace, model_azure, endpoint_name, inference_config):\n  deployment_config = AciWebservice.deploy_configuration(cpu_cores = 1, memory_gb = 1, auth_enabled=True)\n  service = Model.deploy(workspace, endpoint_name, [model_azure], inference_config, deployment_config, overwrite=True)\n  service.wait_for_deployment(show_output = True)\n\n  print(f\"Endpoint : {endpoint_name} was successfully deployed to ACI\")\n  print(f\"Endpoint : {service.scoring_uri} created\")\n  return service\n  \ndef deploy_aks(workspace, model_azure, endpoint_name, inference_config, aks_name):\n  aks_target = AksCompute(workspace, aks_name)\n  aks_config = AksWebservice.deploy_configuration()\n\n  aks_service = Model.deploy(workspace=workspace,\n                             name=endpoint_name,\n                             models=[model_azure],\n                             inference_config=inference_config,\n                             deployment_config=aks_config,\n                             deployment_target=aks_target,\n                             overwrite=True)\n\n  aks_service.wait_for_deployment(show_output = True)\n \n  print(f\"Endpoint : {endpoint_name} was successfully deployed to AKS\")\n  print(f\"Endpoint : {aks_service.scoring_uri} created\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ea3e3f32-e837-4833-b33d-c2028a092360"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"utils","dashboards":[],"language":"python","widgets":{},"notebookOrigID":3715826895427780}},"nbformat":4,"nbformat_minor":0}
