{"cells":[{"cell_type":"markdown","source":["## Deploy churn prediction model\nIn this notebook we will demonstrate how to get the model generated [here]() to deploy it. We need to follow these steps:\n\n- Get an already trained model\n- Instantiate an Azure ML Workspace\n- Build an image with the best model packaged\n- Deploy the model to ACI (Azure Container Instance)\n- Deploy the model to AKS (Azure Kubernetes Services)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c1e4e7e2-6a63-493b-9b27-bc3afe8ff405"}}},{"cell_type":"markdown","source":["## First lets get the model\nReturn the best model from `churn-prediction` experiment. We will use the same notebook **model-churn-prediction** and return the `model_uri`."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3006dd14-9eaf-4eab-9a80-95b3c27179e5"}}},{"cell_type":"code","source":["%run ./model-churn-prediction"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fdf14873-f10f-47c1-aa42-6a15970d3f12"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["And load the `xgboost` using the `model_uri` returned from MLFlow tracking."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f67b27e6-0a59-40da-bca8-3e63b5ff9e4b"}}},{"cell_type":"code","source":["import mlflow\n\nmodel = mlflow.xgboost.load_model(model_uri)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a00fd3a4-aecd-4142-8c9c-11361bebaa65"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Get Azure Machine Learning Workspace\nWe will use Azure Machine Learning to deliver the API `endpoints` that will consume the Machine Learning models. To be able to interact with Azure ML we will use [Azure Machine Learning Python SDK](https://docs.microsoft.com/en-us/python/api/overview/azure/ml/?view=azure-ml-py), with it its possible to create new workspaces (or use existing ones) to facilitate the deployment process.\n\nIts required to fill the variables `WORKSPACE_NAME`, `WORKSPACE_LOCATION`, `RESOURCE_GROUP` and `SUBSCRIPTION_ID` with your subscription data.\n\nAs default will be required the `Interactive Login` auth. For production scenarios an app registration with `Service Principal` is required. In the [documentation] (https://docs.microsoft.com/en-us/azure/machine-learning/how-to-setup-authentication#set-up-service-principal-authentication) we have more details about the different kind of authentications."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"139ad7c6-8c59-4508-9094-490c791fa81d"}}},{"cell_type":"markdown","source":["First install the [`azureml-sdk`](https://pypi.org/project/azureml-sdk/)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"954fcb98-c523-4af4-8186-c2864807a83e"}}},{"cell_type":"code","source":["%sh\n\npip install azureml-sdk"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"349fe25a-2c4f-45b5-85c8-7c7892bc3d21"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["And now we can use it to instantiate the Azure ML Workspace"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2f35bd39-459c-46f8-a28d-c4b0e820a80a"}}},{"cell_type":"code","source":["import azureml\nfrom azureml.core import Workspace\nimport mlflow.azureml\n\nworkspace_name = '<YOUR-WORKSPACE-NAME>'\nresource_group = '<YOUR-RESOURCE-GROUP>'\nsubscription_id = '<YOUR-SUBSCRIPTION-ID>'\n\nworkspace = Workspace.get(name = workspace_name,\n                          resource_group = resource_group,\n                          subscription_id = subscription_id)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"89c676d1-307f-459c-b28a-59021c99e8aa"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Register the model\nNow we instantiate the Azure ML Workspace we can register the model. First we will persist it to the dbfs (to be able to pass the path as a parameters to Azure ML Register)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b2e08801-c5b9-462d-a5eb-dfd90cbabee7"}}},{"cell_type":"code","source":["import shutil\nmodel_path = '/dbfs/models/churn-prediction'\n\n# Delete old files if necessary\ntry:\n  shutil.rmtree(model_path)\nexcept FileNotFoundError:\n  print (\"Cleanig model directory: {} \\nThis directory hasn't been created yet.\".format(model_path))\nelse:\n    print (\"Cleanig model directory: {} \\nDirectory successfully cleaned.\".format(model_path))\n  \n# Persist the XGBoost model\nmlflow.xgboost.save_model(model, model_path)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"463aedfa-3b5f-4f33-ac01-6366840c4128"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["from azureml.core.model import Model\n\nmodel_name = 'churn-model'\nmodel_description = 'Modelo de predição de churn utilizando XGBoost'\n\nmodel_azure = Model.register(model_path = model_path,\n                             model_name = model_name,\n                             description = model_description,\n                             workspace = workspace,\n                             tags={'Framework': \"XGBoost\", 'Tipo': \"Classificação\"}\n                             )"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7ee90d72-4395-4f59-9758-6ba8045b5d1e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["A new model version was generated in the Azure ML Workspace. We can use it to deploy an API with ACI or AKS."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"36f69111-a164-4060-9738-fd8dffa55d9a"}}},{"cell_type":"markdown","source":["#Deploy\nNow with the model registered we can choose between two deployment types: `ACI` (Azure Container Instance) or `AKS` (Azure Kubernetes Service).\n\nFor development scenarios it is better to use `ACI` and for production `AKS` will have more options related to scalability and security. Please see more details in this [page](https://docs.microsoft.com/en-us/azure/architecture/reference-architectures/ai/mlops-python)."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2f8c5f20-3a7d-4d48-93ac-0394d5dfd7d1"}}},{"cell_type":"markdown","source":["### Entry script\nBut before deploy the model, it is important to define an **`entry script`** named score.py. It will be responsible to load the model when the deployed service starts and for receiving data, passing it to the model, and then returning a response as well (see this [link](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-deploy-existing-model#define-inference-configuration))."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cfd1d4c0-ceb1-4737-87e3-37c961762a21"}}},{"cell_type":"code","source":["%%writefile /dbfs/models/churn-prediction/score.py\n\nimport mlflow\nimport json\nimport pandas as pd\nimport os\nimport xgboost as xgb\nimport time\n\n# Called when the deployed service starts\ndef init():\n    global model\n    global train_stats\n\n    # Get the path where the deployed model can be found.\n    model_path = os.path.join(os.getenv('AZUREML_MODEL_DIR'), './churn-prediction')\n    \n    # Load model\n    model = mlflow.xgboost.load_model(model_path)\n\n# Handle requests to the service\ndef run(data):\n  # JSON request.\n  # {\"Cylinders\":0, \"Displacement\":0.0, \"Horsepower\":0.0, \"Weight\":0.0, \"Acceleration\":0.5, \"Model Year\":0, \"USA\":0.0, \"Europe\":0.0, \"Japan\":0.0}\n  \n  info = {\"payload\": data}\n  print(json.dumps(info))\n    \n  data = pd.read_json(data, orient = 'split')\n  data_xgb = xgb.DMatrix(data)\n\n  # Return the prediction\n  prediction = predict(data_xgb)\n  print (\"Prediction created at: \" + time.strftime(\"%H:%M:%S\"))\n  \n  return prediction\n\ndef predict(data):\n  prediction = model.predict(data)[0]\n  return {\"churn-prediction\": str(prediction)}"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1be49f3c-b93f-4d4d-998b-d97b3f12db51"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Inference config\nWe must now add some inference configs to be used in the endpoint. We can add required packages and an environment that can be registered in the Azure ML Workspace.\n\nHere we will use the same `conda.yaml` file that is already registered from MLFlow process. We will add the `azureml-defaults` package that can be used in the inference process."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1693c3c5-3de3-4b12-9f58-956ac65032d2"}}},{"cell_type":"code","source":["from azureml.core.model import InferenceConfig\nfrom azureml.core.environment import Environment\nfrom azureml.core.conda_dependencies import CondaDependencies\n\n# Create the environment\nenv = Environment(name='xgboost_env')\n\nconda_dep = CondaDependencies('/dbfs/models/churn-prediction/conda.yaml')\n\n# Define the packages needed by the model and scripts\nconda_dep.add_pip_package(\"azureml-defaults\")\n\n# Adds dependencies to PythonSection of myenv\nenv.python.conda_dependencies=conda_dep\n\ninference_config = InferenceConfig(entry_script=\"/dbfs/models/churn-prediction/score.py\",\n                                   environment=env)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d148093a-0fb4-4273-9a10-e72906b5b3a1"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Now with the inference config we can proceed with the deployment"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3e92f716-8a7c-4b2e-a47c-545e423b2f97"}}},{"cell_type":"markdown","source":["###ACI - Azure Container Instance\nFollow we will demonstrate how to create an `endpoint` using the image created before and delivering with `ACI`."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8fb6f71f-21f6-44a8-8054-b4d52a969a48"}}},{"cell_type":"code","source":["from azureml.core.webservice import AciWebservice, Webservice\nfrom azureml.exceptions import WebserviceException\nfrom azureml.core.model import Model\n\nendpoint_name = 'api-churn-dev'\n\ndeployment_config = AciWebservice.deploy_configuration(cpu_cores = 1, memory_gb = 1)\nservice = Model.deploy(workspace, endpoint_name, [model_azure], inference_config, deployment_config, overwrite=True)\nservice.wait_for_deployment(show_output = True)\n\nprint('A API {} foi gerada no estado {}'.format(service.scoring_uri, service.state))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f6b153c0-c545-4733-80aa-7c572b89b897"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Load some data to test the endpoint\nWe will use the same dataset used to train the model only for testing purposes."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c58d0ebd-ece4-4782-9a97-41dde98c1ff1"}}},{"cell_type":"code","source":["import requests\n\npayload1='{\"columns\":[\"Idade\",\"RendaMensal\",\"PercentualUtilizacaoLimite\",\"QtdTransacoesNegadas\",\"AnosDeRelacionamentoBanco\",\"JaUsouChequeEspecial\",\"QtdEmprestimos\",\"NumeroAtendimentos\",\"TMA\",\"IndiceSatisfacao\",\"Saldo\",\"CLTV\"],\"data\":[[21,9703,1.0,5.0,12.0,0.0,1.0,100,300,2,6438,71]]}'\n\npayload2='{\"columns\":[\"Idade\",\"RendaMensal\",\"PercentualUtilizacaoLimite\",\"QtdTransacoesNegadas\",\"AnosDeRelacionamentoBanco\",\"JaUsouChequeEspecial\",\"QtdEmprestimos\",\"NumeroAtendimentos\",\"TMA\",\"IndiceSatisfacao\",\"Saldo\",\"CLTV\"],\"data\":[[21,9703,1.0,5.0,12.0,0.0,1.0,1,5,5,6438,71]]}'"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ee20fc20-51de-4b39-a9f9-acd5e2c69127"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Call the API\nMake a request to the API using `query_input`. The API url can be obtained throught `dev_webservice.scoring_uri` generated from deployment process."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"78c4a40e-4b97-489b-b007-ad5acad7febc"}}},{"cell_type":"code","source":["headers = {\n  'Content-Type': 'application/json'\n}\n\nresponse1 = requests.request(\"POST\", service.scoring_uri, headers=headers, data=payload1)\nresponse2 = requests.request(\"POST\", service.scoring_uri, headers=headers, data=payload2)\n\nprint(response1.text)\nprint(response2.text)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"171c117a-52a7-4249-907c-0d9a60cf1352"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["It is also possible to use API using any client to make HTTP requests (curl, postman, etc.)."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5c42236b-7301-4dc1-b891-6424f5fc8f57"}}},{"cell_type":"markdown","source":["## Azure Kubernetes Services (AKS)\nFor production scenarios it is better to deploy using AKS because we have more benefits about security and scalability.\n\nIn this scenario is possible to follow two ways: Creating a new AKS cluster or targeting to an existing one. In this tutorial we will create a new cluster."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4f475584-4235-4678-b82c-8a11e8e9219d"}}},{"cell_type":"code","source":["from azureml.core.webservice import AksWebservice\nfrom azureml.core.compute import AksCompute, ComputeTarget\n\naks_name = 'aks-e2e-ds'\n\nprov_config = AksCompute.provisioning_configuration()\n\naks_target = ComputeTarget.create(workspace = workspace, name = aks_name, provisioning_configuration = prov_config)\n\n#If you want to use an existing AKS cluster, comment the previous command line e un-comment the next one:\n#aks_target = AksCompute(workspace, aks_name)\n\naks_target.wait_for_completion(show_output = True)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b9ce189f-7dfe-4a3f-be77-f138b4de8d62"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["# Deleting aks cluster\n#aks_target = ComputeTarget(workspace=workspace, name=aks_name)\n#aks_target.delete()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9bc9a94d-c4b3-49fc-8785-1f7493eec72c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["endpoint_name = 'api-churn-prod'\n\naks_config = AksWebservice.deploy_configuration(compute_target_name=aks_name)\n\naks_service = Model.deploy(workspace=workspace,\n                           name=endpoint_name,\n                           models=[model_azure],\n                           inference_config=inference_config,\n                           deployment_config=aks_config,\n                           deployment_target=aks_target)\n\naks_service.wait_for_deployment(show_output = True)\nprint(aks_service.state)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fcdb58af-90b2-4403-99e8-42bfbb0567cd"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Call the API (with AKS)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6154bf9c-f936-42bc-ad96-834333ce453b"}}},{"cell_type":"code","source":["prod_service_key = aks_service.get_keys()[0] if len(aks_service.get_keys()) > 0 else None\n\nheaders[\"Authorization\"] = \"Bearer {service_key}\".format(service_key=prod_service_key)\n\nresponse1 = requests.request(\"POST\", aks_service.scoring_uri, headers=headers, data=payload1)\nresponse2 = requests.request(\"POST\", aks_service.scoring_uri, headers=headers, data=payload2)\n\nprint(response1.text)\nprint(response2.text)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9ccbf703-2272-4fbf-a1d7-8068190b05c4"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d1a03e30-90b5-4554-8688-d7f7994003e3"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"deploy-model-churn-prediction","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2,"experimentId":"3147030825313945"},"language":"python","widgets":{},"notebookOrigID":1328906599956139}},"nbformat":4,"nbformat_minor":0}
